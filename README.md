## Purpose(s) of this code

This repo contains a suite of Python tools that make it possible to reprocess RNA-seq data from scratch from a GEO, ENA or SRA study ID spotted in an article, or to retrieve derived results, basically with a few commands. The primary motivation for this code is to draft a scientific manuscript that will scrutinize many scientific articles, so the code is intended to minimize the need to remember relevant details.

The metadata module simplifies the process of determining what fastq files are available, and simplifies the review and selection   and makes separate scripts separately supplementary data in GEO (from the authors or NCBI). The pipeline module handles fastqs transfers, runs alignments, counts, etc., and optionally runs follow-up steps for one or more genes (local reassembly, small BAM file, etc.). 

The methods are standard and open-source, but the data needs be located, and the source of the data matters. For some studies, there is data in SRA or GEO but not both, or only in ENA. The ideal source is ENA since it lets us use Aspera for download. So another function of this code is to find the sample metadata and associate it with the sequencing data so it's easy to review. It keeps the original metadata files intact, and makes separate output files for human and mouse data. 

One of the earliest steps baked into the alignment pipeline is to generate Bash scripts to actually run commands. It keeps the scripts, logs and fastq file manifests with the outputs, and zips them if all goes well. One exception to be corrected is the postprocessing of gene counts.

There may be additional results available in GEO for a study, and there are functions that create Bash scripts to get these files if they exists, separately for data posted by the authors and for results generated by NCBI for human studies ([example](https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE119538), [explanation](https://www.ncbi.nlm.nih.gov/geo/info/rnaseqcounts.html)).
. 
There's a good chance that gene counts are already available separately thanks to the [Ma'ayan lab](https://labs.icahn.mssm.edu/maayanlab/) ([download](https://maayanlab.cloud/archs4/download.html), [awesome Python toolkit](https://github.com/MaayanLab/archs4py/)). This is not yet automated.


This is not a general-purpose pipeline. code is specifically intended t
Many of these steps  generate Bash scripts for transparency and convenience, and they are normally run in sequence but are accessible separately.

### Motivation for this effort

1. First and foremost, the objective is to obtain and review details that are not available even if the data has been reprocessed.
2. Nextflow is intended for large-scale runs, and it would require others to use Nextflow. It has complex capabilities and generates lots of log files not needed here since the goal is simplicity.
3. But really, this is for the parents out there whose kids have cancer.

### Workflow

This should be a diagram, but here is a summary of the workflow:

0. Retrieve and set up reference genome & transcriptome, extract gene coordinates and other ancillary files, etc. (All versions etc. are explicit and noted.)

1. Starting from a study ID reported in an article, typically a GEO series ID (GSE####), find the metadata available in GEO, SRA and ENA for samples and fastq files. Uses a combination of [pySRAdb](https://github.com/saketkc/pysradb) and direct requests.
1. Keep the original metadata files but filter, reformat and separate by species (mouse or human) and experiment type (bulk RNA-seq). Some authors have made this a royal pain.
1.  Merge sample and fastq file details; simplifies the contents, drops redundant columns, calculates the actual read length and type.
1. Choose the samples to re-analyze, which amounts to taking a subset of rows and columns from the last step. The result is a simple manifest (three columns for paired reads, two for single reads), with flexibility that's not spelled out. There can be multiple rows per sample, samples can be renamed,etc.
1. Make a read-length-specific STAR index for the target species if we don't have one already.
1. Make a BED file of the target gene(s) to post-process, technically optional but frequent. This command needs the species and gene name(s), but it's case-insensitive and it can optionally extend the boundaries; this distance and the species get baked into the name of the scripts that gets these boundaries.
1. We need to run one more command to generate all the scripts that will retrieve, align etc.
    - Required:
      - the path to the manifest.
      - a directory with the STAR index to use.
    - Optional:
      - whether to run Abra2 and/or RNAspades;
      - whether and where to move the results when they're done;
      - CPUs and/or max mem for each step, where to get the fastqs (all normally based on fastq URLs, usually ENA) and how fast; everything has a default.

This last command checks for errors to the extent possible, then creates all the temporary and permanent folders, generates separate scripts for post-alignment steps (aggregation/transformation of individual counts files, Abra2, RNASpades) and a main Bash scrip that will run retrieve/align/quantify call the post-processing scripts and clean up logs etc., with error checking along the way, and ends with results for a group of related samples in one dir.
1. Optional: if the study authors posted any files in GEO (i.e., derived results), generate a script to retrieve them.
1. Optional: if there are NCBI counts from GEO (human only, not all studies), generate a separate script to retrieve them. Each of these scripts will download the remote files to the directory where it resides.

Abra2 (local realignment) and RNAspades (reassembly using gene-specific reads + unaligned reads) are separate functions that can run anytime.

## Status

Updates underway to shorten the path to publication and make it possible for others to run this code:
- separated existing functions into modules (metadata, alignment/quantification/assembly pipeline, extract specifc results from different files, making figures etc.), instead of one long script;
- converting argument handling to cyclopts, like so for metadata:

    <a href="docs/metadata_commands.png"><img src="docs/metadata_commands.png"></a>

Some of these steps run in succession automatically.


## Acknowledgments

- All the diligent people who work at NCBI and ENA.
- The authors of STAR, featureCounts, Trackplot, samtools, pySRAdb, Archs4, and more.
- The study authors who posted their reads in GEO, SRA or ENA.

## Limitations/quirks

Some steps and assumptions in this pipeline are admittedly not practices I would use at work. For most studies of interest, read type and length are usually the same for all samples, and fastq files are often available from ENA, so I opted for simplicity. In a given alignment run/batch (i.e., retrieve reads, align, quantify, realign locally, assemble locally, etc.):

  - the same STAR index is used for all samples;
  - the index is specified manually on the command line (but then recorded in a script);
  - there is no explicit test of the read length, I think
  - Fastq URLs are formatted according to the source, which is convenient  but really unnecessary.
- The code that aggregates/reformats/transforms featureCounts output files involves multiple calls and doesn't get copied with the resuls.

